DATASETS:
  # Source - hint3 / dialoglue
  DATASET_SOURCE: "hint3"
  # For HINT3 - curekart / powerplay11 / sofmattress
  # For Dialogue - banking / clinc / hwu
  DATASET_NAME: "curekart"
  OOS_CLASS_NAME: "NO_NODES_DETECTED"
  # Subsets of train data to use. hint3 - "train"/"subset_train", dialoglue - "train_5"/"train_10"
  DATA_SUBSET: "train"
  N_LABELS: 28

# Training parameters only for BERT based models
TRAINING:
  SUB_SAMPLE_QQ : True # If subsampling is needed for Question Pair generation
  SAMPLE_SIZE_PER_DATASET: 100000
  VAL_SPLIT : 0.1
  GENERATE_TRIPLETS : False
  HARD_SAMPLE : True # Used with Sub-sampling
  # Model - type -  BI_ENCODER / BERT_CLASSIFIER / SBERT_CROSS_ENCODER
  MODEL_TYPE : "BI_ENCODER"
  NUM_ITERATIONS : 10000
  TRAIN_OUTPUT_DIR: "./models/"
  # Specify the model like "sentence-transformers/all-MiniLM-L6-v2" or "bert-base-uncased" /"cross-encoder/stsb-distilroberta-base"
  MODEL_NAME : "sentence-transformers/all-MiniLM-L6-v2"
  TOKENIZER_NAME: "sentence-transformers/all-MiniLM-L6-v2"
  # Specify model layer to unfreeze - 11,5 etc
  LAYERS_TO_UNFREEZE : [5]
  # Specify a loss metric for Sentence Bert Bi-Encoder models - ContrastiveLoss / BatchHardTripletLoss
  LOSS_METRIC : "ContrastiveLoss"
  BATCH_SIZE : 16
  LEARNING_RATE: 2e-5
  # Specify scheduler - "WarmupLinear" for SentenceBert / "linear" for Bert
  SCHEDULER: 'WarmupLinear'
  VALIDATION_SPLIT: 0.1

# Evaluation for all methods including pre-trained models, BM25, Glove, Fasttext, finedtuned models etc
EVALUATION:
  # Evaluation methods - BERT_EMBEDDINGS / BERT_CLASSIFIER / SBERT_CROSS_ENCODER / BM25 / GLOVE / FASTTEXT / TFIDF_WORD_EMBEDDINGS / TFIDF_CHAR_EMBEDDINGS / CV_EMBEDDINGS
  EVALUATION_METHOD : "SBERT_CROSS_ENCODER"
  # Model name for evaluation - # Specify a model name which is implemented via Huggingface - "bert-base-uncased" / "models/convbert" (DialoGLUE convbert model) / "sentence-transformers/all-MiniLM-L6-v2"
  MODEL_NAME : "bert-base-uncased"
  TOKENIZER_NAME: "bert-base-uncased"
  BATCH_SIZE : 16
  # Glove & Fastext model paths
  FASTTEXT_MODEL_PATH: "models/fasttext_ecom_model_2.bin"
  GLOVE_MODEL_PATH: "models/glove.6B/glove.6B.300d.txt"
  # Set True/False for the evaluation metrics
  CHECK_SUCCESS_RATE: True
  CHECK_PRECISION : True
  CHECK_MAP: True
  CHECK_NDCG: True
  CHECK_MRR: True
  CHECK_F1_MACRO: True
  CHECK_F1_MICRO: True
  CHECK_F1_WEIGHTED: True
  CHECK_OOS_ACCURACY : True
  OOS_THRESHOLD : [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]
  # Specify k values less than 10
  K_VAL : [1,2,3,5]


# Training parameters only for BERT based models
PRETRAINING:
  SAMPLE_SIZE_PER_DATASET: 100000
  VAL_SPLIT : 0.1
  GENERATE_TRIPLETS : False
  GENERATE_PAIRS: False
  HARD_SAMPLE : False
  PRETRAIN_DATA_PATH: "data/pretrain"
  STEPS_PER_EPOCH : 140000
  NUM_TRAIN_EPOCHS : 1
  TRAIN_OUTPUT_DIR: "./models/"
  # Specify the model like "sentence-transformers/all-MiniLM-L6-v2" / "sentence-transformers/all-mpnet-base-v2"
  MODEL_NAME : "sentence-transformers/all-MiniLM-L6-v2"
  # Specify a loss metric for Sentence Bert Bi-Encoder models - ContrastiveLoss / TripletLoss
  LOSS_METRIC : "TripletLoss"
  BATCH_SIZE : 32
  LEARNING_RATE: 2e-5
  # Specify scheduler - "WarmupLinear" for SentenceBert / "linear" for Bert
  SCHEDULER: 'WarmupLinear'


INFERENCE:
  TENANT_NAMES : ["curekart", "powerplay11"]
  INFERENCE_FILE_PATH: "data/inference.txt"
  MODEL_DIR: "models"
  MODEL_NAME: "sentence-transformers/all-MiniLM-L6-v2"
  EMBEDDING_FILE_NAME: "embeddings.npy"
  TEXTS_FILE_NAME: "texts.npy"
  LABELS_FILE_NAME: "labels.npy"
  LAYERS_TO_LOAD: [5]
